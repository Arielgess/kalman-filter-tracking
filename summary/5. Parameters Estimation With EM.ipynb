{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf5d052",
   "metadata": {},
   "source": [
    "# Parameter Estimation with Expectation-Maximization (EM)\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm is a method for estimating unknown parameters in probabilistic models when some variables are unobserved. In the context of Kalman Filters, we will use EM to estimate Q and R from data, without knowing the true underlying states.\n",
    "\n",
    "This is particularly valuable because in real-world applications, we often don't know the true noise characteristics of our system, but we can learn them from the data itself.\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "In a Kalman Filter, we need to specify:\n",
    "- Q matrix: Process noise covariance - how much uncertainty we expect in the system dynamics.\n",
    "- R matrix: Observation noise covariance - how much uncertainty we expect in our measurements\n",
    "\n",
    "These parameters significantly affect the filter's performance, but they're often unknown in practice. By using the EM algorithm, we can have a good estimation of them from the data.\n",
    "We must note that in order to estimate them properly, we need data with the same underlying real process noise and observation noise."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Brief Introduction To Smoothing\n",
    "We already talked briefly about smoothing in notebook 2. However, we would go a little deeper in the technical details in order to understand better how the EM works in our case of the KF.\n",
    "\n",
    "Specifically, we would talk about a smoothing algorithm called RTS (Rauch-Tung-Striebel).\n",
    "\n",
    "The RTS smoother works by:\n",
    "1. Running the Kalman Filter forward to get filtered estimates of the states and covariances (both a-preori and a-postiriori).\n",
    "2. Running a backward pass to incorporate future information into past filtered estimates.\n",
    "\n",
    "The forward pass is the basic predict -> update loop that we already covered in the KF.\n",
    "\n",
    "The backward pass uses:\n",
    "$$\\hat{x}_{k|T} = \\hat{x}_{k|k} + J_k (\\hat{x}_{k+1|T} - \\hat{x}_{k+1|k})$$\n",
    "\n",
    "where $J_k$ is the smoothing gain that tells us how much to trust the future information.\n",
    "The smoothing gain $J_k$ is computed as:\n",
    "$$J_k = P_{k|k} F^T P_{k+1|k}^{-1}$$\n",
    "\n",
    "(We already know this notation from notebook 2)\n",
    "The smoothing gain tells us how much to adjust our current estimate $\\hat{x}_{k|k}$ based on future information (The a-posteriori covariance matrix of the next state). It's proportional to:\n",
    "- How uncertain we were about the state at time $k$ ($P_{k|k}$)\n",
    "- How much the state at time $k$ affects the state at time $k+1$ ($F^T$)\n",
    "- How much we trust our prediction at time $k+1$ ($P_{k+1|k}^{-1}$)\n",
    "\n",
    "The intuition is that if $J_k$ is large, it means that the next state has extra information about $x_k$ than what we already know, so by taking the information that we got from smoothing, we would get a better estimation for $x_k$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d00084b7c3c9580e"
  },
  {
   "cell_type": "markdown",
   "id": "98e733c0",
   "metadata": {},
   "source": [
    "## How EM Works\n",
    "\n",
    "The EM algorithm is an iterative method that alternates between two steps:\n",
    "\n",
    "## E-step (Expectation)\n",
    "Given current estimates of Q and R, we compute the expected values of the hidden states.\n",
    "Intuitively, we will get the most accurate expected values of them, if we will use all the observed data.\n",
    "Meaning, given all the observed data, what would we expect the hidden states to be. \n",
    "This is exactly what smoothing does (We mentioned it briefly in notebook 2).\n",
    "\n",
    "Given measurements $y_1, y_2, \\ldots, y_T$, we want to estimate the hidden state $x_k$ at time $k$ using all the measurements.\n",
    "\n",
    "The smoothed state estimate is:\n",
    "$$\\hat{x}_{k|T} = E[x_k | y_1, y_2, \\ldots, y_T]$$\n",
    "\n",
    "And the smoothed covariance is:\n",
    "$$P_{k|T} = \\text{Cov}[x_k | y_1, y_2, \\ldots, y_T]$$\n",
    "\n",
    "In the EM algorithm, we need to compute expectations like:\n",
    "- $E[x_k | y_1, \\ldots, y_T]$ (the smoothed state)\n",
    "- $E[x_k x_k^T | y_1, \\ldots, y_T]$ (for covariances)\n",
    "- $E[x_k x_{k-1}^T | y_1, \\ldots, y_T]$ (for pairwise covariances)\n",
    "\n",
    "These are exactly what the RTS smoother computes. The smoother gives us:\n",
    "- $\\hat{x}_{k|T}$ = $E[x_k | y_1, \\ldots, y_T]$\n",
    "- $P_{k|T}$ = $E[x_k x_k^T | y_1, \\ldots, y_T] - \\hat{x}_{k|T} \\hat{x}_{k|T}^T$\n",
    "- $V_{k,k-1}$ = $E[x_k x_{k-1}^T | y_1, \\ldots, y_T] - \\hat{x}_{k|T} \\hat{x}_{k-1|T}^T$\n",
    "\n",
    "\n",
    "## M-step (Maximization)  \n",
    "Given the expected values, we update Q and R to maximize the likelihood of the observed data.\n",
    "\n",
    "#### R Matrix Update\n",
    "The R matrix represents the uncertainty in our measurements. We update it using (from the paper https://www.stat.pitt.edu/stoffer/dss_files/em.pdf):\n",
    "$$R_{\\text{new}} = \\frac{1}{T} \\sum_{t=1}^T \\left[ (y_t - H \\hat{x}_{t|T})(y_t - H \\hat{x}_{t|T})^T + H P_{t|T} H^T \\right]$$\n",
    "\n",
    "#### Q Matrix Update (Process Noise)\n",
    "The Q matrix represents the uncertainty in our system dynamics. We update it using:\n",
    "\n",
    "$$Q_{\\text{new}} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\left[ (\\hat{x}_{t+1|T} - F \\hat{x}_{t|T})(\\hat{x}_{t+1|T} - F \\hat{x}_{t|T})^T + F P_{t|T} F^T + P_{t+1|T} - V_{t+1,t} F^T - F V_{t+1,t}^T \\right]$$\n",
    "\n",
    "\n",
    "The algorithm iterates between these steps until convergence.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Step-Size For Updates\n",
    "In the empirical tests I did, I saw that there could be rather big jumps of values when estimating R and Q.\n",
    "I saw that I got better estimations when I calculated R and Q while incorporating \"steps\" towards the current calculated values. Meaning, after an iteration of the EM, instead of the assignment above of $Q_{\\text{new}}, R_{\\text{new}}$, I updated them with some average between the newly calculated and the old ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c65aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a23e65",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
