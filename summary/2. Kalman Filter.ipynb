{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Summary Goal ##\n",
    "This would be a very high level overview of the Kalman Filter.\n",
    "I will focus here on examples, intuition and high-level understanding. We will not dive into the statistics behind the Kalman Filter. Note that in order to use it - we also don't need to (Although from the little I read, it's very interesting!)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b89c8f5714cbad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is the Kalman Filter?\n",
    "\n",
    "The Kalman Filter (KF) is an algorithm that allows us to estimate the hidden state of a system over time.  \n",
    "It combines two sources of information:\n",
    "\n",
    "1. **The model** – how we believe the system evolves (the state-space equations).  \n",
    "2. **The measurements** – noisy observations of the system.  \n",
    "\n",
    "By recursively blending these two, the KF produces estimates that are usually more accurate than relying on either the model or the measurements alone.\n",
    "There is an internal uncertainty in the Kalman Filter, which is usually shown as the matrix P. That uncertainty is the internal state covariance. The whole idea of the kalman filter is that as the k get bigger (meaning, as we advance in time and exposed to more measurements), P should get smaller and smaller. If it does, it means that the Kalman Filter is doing its work, and its able to detect well the underlying signal (without the noise) of the system it tries to estimate.\n",
    "<br><br>\n",
    "\n",
    "## How does it work?\n",
    "The Kalman Filter has two main methods - predict, and update.\n",
    "\n",
    "**Predict step**\n",
    "we use \"predict\" to predict the next state, without observing the actual measurement that corresponds to that state.\n",
    "We also predict the next stat's Covariance matrix. As stated before, if the Kalman Filter's model fits the motion model, we expect it to get smaller and smaller.\n",
    "\n",
    "$$\n",
    "\\hat{x}_{k|k-1} = A \\hat{x}_{k-1|k-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{k|k-1} = A P_{k-1|k-1} A^\\top + Q\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_{k|k-1} = C \\hat{x}_{k|k-1}\n",
    "$$\n",
    "\n",
    "- $\\(\\hat{x}_{k|k-1}\\)$: predicted state estimate at time \\(k\\) (before seeing \\(y_k\\))  \n",
    "- $\\(A\\)$: state transition matrix. In our case, $A$ will represent a physical model of movement (For example, constant-velocity model)  \n",
    "- $\\(Q\\)$: process noise covariance  \n",
    "- $\\(u_k\\)$: Process Noise. It is a gaussian random variable, distributed \\mathcal{N}(0, Q) \n",
    "- $\\(P_{k|k-1}\\)$: predicted state covariance (uncertainty) for state k, given information on the first k-1 steps\n",
    "- $\\hat{y}_{k|k-1}$: The estimation of the measurement of the model \n",
    "- $C$: observation matrix  \n",
    "\n",
    "\n",
    "**Update step**\n",
    "When the new measurement arrives, we update the prediction:\n",
    "$$K_k = P_{k|k-1} C^\\top (C P_{k|k-1} C^\\top + R)^{-1}$$\n",
    "$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - C \\hat{x}_{k|k-1})$$\n",
    "$$P_{k|k} = (I - K_k C) P_{k|k-1}$$\n",
    "\n",
    "- $K_k$: Kalman gain — balance between trusting the model vs. the measurement (analogous to the alpha in a LP filter) \n",
    "- $y_k$: observation at time \\(k\\)  \n",
    "- $C$: observation matrix  \n",
    "- $R$: observation/measurement noise covariance  \n",
    "- $\\hat{x}_{k|k}$: updated state estimate after seeing $y_k$  \n",
    "- $P_{k|k}$: updated covariance for step k\n",
    "\n",
    "\n",
    "We call this method iteratively - \n",
    "**predict** -> **update** -> **predict** -> **update** ...\n",
    "Call predict when we want to estimate the next state\n",
    "Call update when we have the actual next measurement, and it \"calibrates\" the KF according to the mistake\n",
    "\n",
    "<br><br>\n",
    "## Usages of the Kalman Filter\n",
    "As stated before, the main goal of the KF is to produce estimators for the underlying unobserved signal over time.\n",
    "Suppose the underlying signal at time t is $x_t$, and we are using the measurements $Y_s = {y_1, y_2, \\dots,y_s}$ to estimate it.\n",
    "When s < t, the problem is called forecasting.\n",
    "When s = t,the problem is called filtering.\n",
    "When s > t, the problem is called smoothing.\n",
    "<br>\n",
    "\n",
    "### 1. Forecasting (Prediction)\n",
    "Estimate a future underlying signal given current information:\n",
    "\n",
    "$$p(x_{k+1} \\mid y_{1:k}) = \\mathcal{N}(\\hat{x}_{k+1|k}, P_{k+1|k})$$\n",
    "\n",
    "- Produces a forecast distribution for the next hidden state.  \n",
    "- Useful when planning or simulating ahead.\n",
    "- At time $k$, you only know the measurements $y_1, \\dots, y_k$.  \n",
    "- Using those, you predict where the system will be at $k+1$.  \n",
    "- This is useful for forecasting. The estimation is more uncertain that filtering, because we have yet to see $y_{k+1}$\n",
    "<br>\n",
    "\n",
    "In our project, we would actually want to check the forecasting after t steps (t-lag prediction). Meaning, we would like to compute $\\hat{x}_{k+t|k}$. In the code we would call it k-lag prediction, but that's the same idea.\n",
    "\n",
    "### 2. Filtering\n",
    "Estimate the current underlying signal given all observations up to now:\n",
    "\n",
    "$$p(x_k \\mid y_{1:k}) = \\mathcal{N}(\\hat{x}_{k|k}, P_{k|k})$$\n",
    "- At time $k$, you’ve just received measurement $y_k$.  \n",
    "- The filter combines the prior prediction $\\hat{x}_{k|k-1}$ with $y_k$ to give the best estimate of the current state.  \n",
    "- This is the real-time mode of the KF: at each step, the filter provides the most up-to-date estimate of the system.\n",
    "- Balances the model’s prediction with the latest measurement.  \n",
    "- The most common KF use. Used for real-time tracking.  \n",
    "<br>\n",
    "\n",
    "### 3. Smoothing\n",
    "Estimate a past underlying signal using both past and future data:\n",
    "\n",
    "$$k < T$$\n",
    "$$p(x_k \\mid y_{1:T}) = \\mathcal{N}(\\hat{x}_{k|T}, P_{k|T})$$\n",
    "\n",
    "\n",
    "- Suppose you want to know what the state was at time $k$, but now you also have measurements up to time $T > k$.  \n",
    "- By using future data, you can refine your estimate of past states.  \n",
    "- This requires running a backward pass after the forward filtering. Many KF framework have that function.  \n",
    "- Smoothing provides the most accurate trajectory but can only be done offline, once the entire dataset is available.\n",
    "- We will mostly use Forecasting and the Filtering, but it's nice to know"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9834759222b2a4ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of Kalman Filter\n",
    "\n",
    "<br><br>\n",
    "### Innovations\n",
    "\n",
    "The **innovation** (also called the residual) is the difference between the actual measurement and the predicted measurement:\n",
    "\n",
    "$$r_k = y_k - \\hat{y}_{k|k-1} = y_k - C \\hat{x}_{k|k-1}$$\n",
    "Or in the t-lag prediction:\n",
    "$$r_{k+t} = y_{k+t} - \\hat{y}_{k+t|k} = y_{k+t} - C \\hat{x}_{k+t|k}$$\n",
    "\n",
    "\n",
    "- $r_k$: innovation (residual) at time $k$  \n",
    "- $y_k$: the real measurement at time $k$  \n",
    "- $\\hat{y}_{k|k-1} = C \\hat{x}_{k|k-1}$: predicted measurement from the model  \n",
    "\n",
    "Intuitively, the innovation measures how “surprising” the new observation is compared to what the model expected.  \n",
    "It is the direct quantity used in the update step to correct the state estimate.\n",
    "\n",
    "The distribution of the innovation is also Gaussian:\n",
    "\n",
    "$$r_k \\sim \\mathcal{N}(0, S_k), \\quad S_k = C P_{k|k-1} C^\\top + R$$\n",
    "\n",
    "- $S_k$: innovation covariance, representing how uncertain the model was about this measurement  \n",
    "\n",
    "If the Kalman Filter is well-specified, the innovations should look like zero-mean white noise.  \n",
    "Large biases or unusually large residuals usually indicate that the motion or noise models ($A$, $Q$, $C$, $R$) are not well matched to the true system.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7307e8b2013a9e6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the filter is running, we want to know if it is performing well and if the assumed models are consistent with the data.  \n",
    "Two common ways to check this are error metrics and statistical tests.\n",
    "\n",
    "<br>\n",
    " \n",
    "### 1. Mean Squared Error (MSE) and Normalized Mean Squared Error (NMSE)\n",
    "If the true hidden state $x_k$ is known (e.g. in simulation), we can compute the average squared error:\n",
    "**a. with true values of the underlying signal**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{T} \\sum_{k=1}^T \\| x_k - \\hat{x}_{k|k} \\|^2\n",
    "$$\n",
    "\n",
    "- Measures how close the estimated states are to the true states.  \n",
    "- Useful for benchmarking in experiments, but not available in real-world data where $x_k$ is hidden.\n",
    "\n",
    "**b. with values of the measurements**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{T} \\sum_{k=1}^T \\| y_k - \\hat{x}_{k|k} \\|^2\n",
    "$$\n",
    "\n",
    "- This can fit real-world applications. However, one must remember that the measurements are noisy, so it is better to use this method as a supplement with NIS which is discussed next.\n",
    "\n",
    "**c. as a normalized version**\n",
    "Sometimes the error is normalized by the scale of the measurements to make it comparable across systems.\n",
    "We note that the noise in the measurements still affects the NMSE, but the normalization makes sure we are not misled by the noise or the signal scale.\n",
    "\n",
    "$$\n",
    "\\text{NMSE} = \\frac{\\text{MSE}}{\\frac{1}{T}\\sum_{k=1}^T \\| y_k - \\bar{y} \\|^2}\n",
    "$$\n",
    "\n",
    "- We normalize by the second moment (variance) of the measurements.\n",
    "- $\\bar{y}$: mean of the measurements  \n",
    "- This gives a relative error measure that accounts for signal energy or variance.\n",
    "\n",
    "\n",
    "### 2. Normalized Innovation Squared (NIS)\n",
    "When the true state is not available, we can check if the innovations behave like white Gaussian noise with the right variance.  \n",
    "For each step:\n",
    "\n",
    "$$\n",
    "\\text{NIS}_k = r_k^\\top S_k^{-1} r_k\n",
    "$$\n",
    "\n",
    "- $r_k$: innovation at step $k$  \n",
    "- $S_k$: innovation covariance  \n",
    "\n",
    "If the filter is consistent, $\\text{NIS}_k$ should follow a chi-squared distribution with degrees of freedom equal to the measurement dimension.  \n",
    "Large or biased values indicate a mismatch between the assumed model and the actual system.\n",
    "\n",
    "\n",
    "### Summary\n",
    "- **MSE**: Best used when true state $x_k$ is known (simulation).\n",
    "- **NMSE**: A normalized version for comparing across datasets.    \n",
    "- **NIS**: Best used when true state is unknown (real sensor data).  \n",
    "\n",
    "These tools help verify that the Kalman Filter is not just producing estimates, but that those estimates are statistically consistent with the assumed system and noise models.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fe496de92815902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
